#!/bin/bash

#SBATCH --job-name=finetune-sweep         # Name of the job, will appear in monitoring tools
#SBATCH --array=0-3%4                     # Create a job array with indices 0 to 3, with a maximum of 4 jobs running concurrently
#SBATCH --time=7-00:00:00                 # Set a time limit for each job in the array (2 hours here)                  # Set a time limit for each job in the array (2 hours here)
#SBATCH --mem=20G                         # Request 20GB of memory for each job
#SBATCH --cpus-per-task=4                 # Request 4 CPU cores per task
#SBATCH --gres=gpu:1                      # Request 1 GPU per job (general resource scheduling)
#SBATCH --output=/home/mdellag/SplitNanoGPT/logs/%x-%A-%a.out  # Standard output log file; %x is job name, %A is job ID, %a is array index
#SBATCH --error=/home/mdellag/SplitNanoGPT/logs/%x-%A-%a.err   # Standard error log file; %x is job name, %A is job ID, %a is array index

# Define array of lr values as variables
# log space from 1e-5 to 1e-3 -> (0.01 0.1 1.0 10.0 100.0)
# divide the temp penalty decays by 653, the average beta value at init
TEMP_PENALTY_DECAYS=(0.0000153 0.000153 0.00153 0.0153)

# Consider what the gradient of the penalty loss is at init
# d/dx str * exp(-decay*|x|) = sign(x) * str * -decay * exp(-decay*|x|)
# So if we want to start off with the same gradient for all, we should change str to be str/decay
# We want str = 0.03
# So we should set str = 0.03 / decay
TEMP_PENALTY_STRS=(1961.0 196.1 19.6 1.961 0.1961)
QK_DECAY=0.0
WEIGHT_DECAY=0.0

ARG_FOR_ALL="--max_iters=25000 --wandb_project=openwebtext-finetune --weight_decay=$WEIGHT_DECAY --split=True --wandb_run_name=exp(-|x|)_temp_penalty_sweep --qk_weight_decay=$QK_DECAY"

cd /home/mdellag/SplitNanoGPT/

# Environment setup
source /home/mdellag/miniconda3/etc/profile.d/conda.sh    # Initialize Conda
conda deactivate                                          # Deactivate any existing environment
conda activate nanoGPTenv                                 # Activate the desired Conda environment


# Extract the specific command based on the SLURM array task ID and ARG_FOR_ALL
command="python3 /home/mdellag/SplitNanoGPT/split_train.py --temp_penalty_str=$${TEMP_PENALTY_STRS[$SLURM_ARRAY_TASK_ID]} --temp_penalty_decay=${TEMP_PENALTY_DECAYS[$SLURM_ARRAY_TASK_ID]}  $ARG_FOR_ALL"

# Echo the command to stdout for transparency
echo "Running command: $command"

# Execute the command
$command